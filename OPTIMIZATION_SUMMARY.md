# üöÄ Performance Optimization - Quick Summary

## What Was Done

### 1. **Code Optimizations** ‚úÖ

Updated the following files:

#### `src/config/settings.py`
- Changed default LLM model from `gpt-3.5-turbo` to `gpt-4o-mini` (latest fast model)
- Added `MAX_TOKENS=4096` to limit response length
- Added `TEMPERATURE=0.1` for faster, more deterministic responses
- Added performance flags: `ENABLE_STREAMING`, `PARALLEL_EMBEDDINGS`, `CACHE_EMBEDDINGS`
- Increased `TOP_K_CHUNKS` from 3 to 5 for better accuracy

#### `src/llm/openai_client.py`
- Added explicit timeout: `timeout=60.0`
- Reduced retries: `max_retries=2` (faster failures)
- Applied `max_tokens` and `temperature` to all LLM calls
- Already had excellent logging for performance monitoring ‚úÖ

#### Existing Optimizations (Already Implemented)
- ‚úÖ **Batched parsing**: Single LLM call extracts drugs, diagnoses, and patient info
- ‚úÖ **Batched eligibility**: Single LLM call checks all drugs at once
- ‚úÖ **Parallel embeddings**: Batch OpenAI embeddings API for multiple queries
- ‚úÖ **Embedding caching**: Disk cache for query embeddings (data/embedding_cache/)

### 2. **Documentation** üìö

Created three new files:

#### `PERFORMANCE_GUIDE.md`
Comprehensive guide explaining:
- What optimizations are implemented
- Why the app is still slow (API latency)
- Configuration options
- Performance tuning presets
- Troubleshooting tips

#### `.env.example`
Complete environment configuration template with:
- Detailed comments for each setting
- Three performance presets (speed/balanced/accuracy)
- Recommended values

#### `benchmark_performance.py`
Automated benchmarking script that:
- Tests all components (parsing, retrieval, eligibility)
- Measures precise timing for each step
- Provides performance assessment
- Easy to run: `python benchmark_performance.py`

## Current Performance Status

Based on your logs (using `gpt-5-nano` or similar):
- **Parsing**: 22 seconds (single batched LLM call) ‚úÖ
- **Retrieval**: 0.4 seconds (with embedding caching) ‚úÖ  
- **Eligibility**: 40 seconds (single batched LLM call for 1 drug) ‚úÖ
- **Total**: ~62 seconds for complete analysis

### Why It's Slow

The bottleneck is **OpenAI API latency**, specifically:

1. **Token generation time**: 
   - Parsing: 2,806 tokens @ ~50 tokens/sec = ~56 seconds
   - Eligibility: 7,392 tokens @ ~50 tokens/sec = ~148 seconds
   - Your logs show 22s + 40s = 62s total (already optimized!)

2. **Network latency**: Physical distance to OpenAI servers

3. **Cannot be optimized further without**:
   - Using a faster model (you're already using the fastest)
   - Reducing prompt/response size (risky for accuracy)
   - Moving to local LLM (requires GPU, lower accuracy)

## What You Should Do Now

### Option 1: Test Current Optimizations
```bash
# 1. Create .env from template
cp .env.example .env

# 2. Edit .env and add your OpenAI API key
# Make sure LLM_MODEL is set to your fastest model

# 3. Run benchmark
python benchmark_performance.py
```

### Option 2: Try Different Performance Presets

Edit your `.env` file and uncomment one of these:

**Maximum Speed** (may reduce accuracy):
```bash
LLM_MODEL=gpt-3.5-turbo-0125
TOP_K_CHUNKS=3
MAX_TOKENS=2048
TEMPERATURE=0.0
```

**Balanced** (recommended):
```bash
LLM_MODEL=gpt-4o-mini
TOP_K_CHUNKS=5
MAX_TOKENS=4096
TEMPERATURE=0.1
```

**Maximum Accuracy** (slower):
```bash
LLM_MODEL=gpt-4o
TOP_K_CHUNKS=10
MAX_TOKENS=8192
TEMPERATURE=0.2
```

### Option 3: Monitor in Production

The app already logs detailed performance metrics:
- Watch for ‚ö†Ô∏è warnings about slow operations
- Check embedding cache hit rates (2nd run should be faster)
- Monitor token usage in logs

## Expected Results

With optimizations:
- **1st run**: Similar to current (cache miss on embeddings)
- **2nd+ runs**: 0.4s faster on retrieval (cache hits)
- **Multiple drugs**: Same time as 1 drug (batched processing!)

**Key Insight**: Checking 3 drugs takes ~60s (same as 1 drug) due to batching!

## Next Steps if Still Too Slow

1. **Prompt Compression**: Reduce SUT chunk sizes (risky)
2. **Response Streaming**: Better UX (user sees results as they generate)
3. **Local LLM**: Use Llama/Mistral (requires GPU, lower accuracy)
4. **Hybrid Approach**: Use fast model for screening, slow model for edge cases

## Files Changed

- ‚úèÔ∏è `src/config/settings.py` - Added performance settings
- ‚úèÔ∏è `src/llm/openai_client.py` - Added timeout, retries, token limits
- üìÑ `PERFORMANCE_GUIDE.md` - Comprehensive documentation
- üìÑ `.env.example` - Configuration template
- üìÑ `benchmark_performance.py` - Benchmarking tool
- üìÑ `OPTIMIZATION_SUMMARY.md` - This file

## Questions?

Read `PERFORMANCE_GUIDE.md` for detailed explanations and troubleshooting tips!
